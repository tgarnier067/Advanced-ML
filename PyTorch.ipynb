{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93e1122-2077-402f-af03-58ae2d23109b",
   "metadata": {},
   "source": [
    "# Coding Session 4 - Basics of PyTorch.\n",
    "\n",
    "\n",
    "In this coding session, we will get some basic practice with PyTorch. PyTorch a very popular library for implementing deep learning models. A critical aspect of PyTorch's functionality is the fact that it supports efficient execution on GPUs, but we will not be using GPUs in this coding session. The code we'll use is heavily borrowed from the [Machine Learning with PyTorch and Scikit-learn book's Github repo](https://github.com/rasbt/machine-learning-book/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a3e9c-e73a-4010-9530-8b9276ac5968",
   "metadata": {},
   "source": [
    "First, we need to install PyTorch. Run the following code from command line:\n",
    "\n",
    "`pip3 install torch torch-vision`\n",
    "\n",
    "Once this has run, the following code should execute on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1342aa-c3bb-403f-9ca0-f987bcb9a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e2d09-d053-47b5-bb4c-1dba08c20425",
   "metadata": {},
   "source": [
    "PyTorch, as any deep learning library, works with *tensors*. Tensors generalize scalars, vectors, and matrices. In particular, each of these objects can be thought of as a map from a product of vector spaces to the real numbers. For example, every vector $v \\in \\mathbb{R}^d$ can be associated to a unique linear map from $\\mathbb{R}^d \\to \\mathbb{R}$ via $x \\mapsto v^T x$. And a matrix $M \\in \\mathbb{R}^{m \\times n}$ can be associated to the unique bi-linear (this means it is linear in each argument separately) map from $\\mathbb{R}^m \\times \\mathbb{R}^n \\to \\mathbb{R}$ which takes $(x, y) \\mapsto y^T Mx$.\n",
    "\n",
    "More abstractly, given a product of vector spaces $V := V_1 \\times V_2 \\times \\cdots \\times V_K$, we say a map $T \\colon V \\to W$ where $W$ is another vector space is *multi-linear* if for all choices of $k \\in [K]$ and fixed\n",
    "$$\n",
    "v_1 \\in V_1, v_2 \\in V_2, \\ldots v_{k - 1} \\in V_{k - 1}, v_{k + 1} \\in V_{k + 1},\\ldots,\n",
    "v_K \\in V_K,\n",
    "$$ the map which takes\n",
    "$$\n",
    "v \\in V_k \\mapsto T(v_1, \\ldots, v_{k - 1}, v, v_{k + 1}, \\ldots, v_K)\n",
    "$$ is linear. Tensors are just another name for multi-linear maps, and the integer $K$ is known as the *rank* of the tensor.\n",
    "\n",
    "Multi-linear maps/tensors arise naturally in multivariable calculus, differential geometry, physics and many other areas. In machine learning, they are not described in such an abstract way but they arise just as naturally, being a clean way of thinking about multi-dimensional arrays. The indices of each of these arrays generally look like `[data set index, feature index 1, feature index 2, ...]`; for example in convolutional neural networks the first feature index indicates the data dimension, and the second feature index indicates the channel (RGB).\n",
    "\n",
    "Let's start by creating and manipulating some tensors in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9efb4-f81e-4e45-aa90-1f799951241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = np.array([4, 5, 6], dtype=np.int32)\n",
    "\n",
    "t_a = torch.tensor(a)\n",
    "t_b = torch.from_numpy(b)\n",
    "\n",
    "print(t_a)\n",
    "print(t_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a253880-c03b-4b48-8164-a524821ac701",
   "metadata": {},
   "source": [
    "We can check if objects are torch tensors via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43342072-ba26-401b-9dce-60eafe04b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.is_tensor(a), torch.is_tensor(t_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56426621-cccc-47bd-8b68-a672b5400e3b",
   "metadata": {},
   "source": [
    "Similar to NumPy we can make all-ones tensors with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90167cf-0059-4830-a9fe-9674366685c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ones = torch.ones(2, 3)\n",
    "print(t_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff2060-202a-431f-a48e-062d60c8ca6f",
   "metadata": {},
   "source": [
    "Think about what the following should return before you run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0bfa2-3f0a-4052-b6f0-e3c71c074a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(torch.ones(2, 3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f249da-d2f9-4eb0-8da3-0989e5b43f56",
   "metadata": {},
   "source": [
    "We can also manipulate tensors in the following way (should be recognizable if you are familiar with NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6362501-517c-475c-9d9d-70492108d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3, 5)\n",
    "\n",
    "# observe carefully that when we work with tensors we need to\n",
    "# specify the axes that we perform the transpose on\n",
    "t_tr = torch.transpose(t, 0, 1)\n",
    "print(t.shape, ' --> ', t_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2e496-0ec9-467d-b098-cab9508fdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(30)\n",
    "\n",
    "t_reshape = t.reshape(5, 6)\n",
    "\n",
    "print(t_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040ad01-2c36-4f73-bb06-c0edb99a80cb",
   "metadata": {},
   "source": [
    "It's also often helpful to get rid of extraneous axes using the squeeze functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89a57e-cb79-4027-9c21-e944e06815b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(1, 2, 1, 4, 1)\n",
    "\n",
    "t_sqz = torch.squeeze(t, 2)\n",
    "\n",
    "print(t.shape, ' --> ', t_sqz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9af669-8e12-4ab8-add4-4229ca4abcb2",
   "metadata": {},
   "source": [
    "It is very important that we understand how to use the basic mathematical operations on tensors. The following code executes element-wise multiplication, summing along an axis, matrix multiplication, and norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88774ee5-164a-495b-ac2c-1b15a5553d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "t1 = 2 * torch.rand(5, 2) - 1\n",
    "t2 = torch.normal(mean=0, std=1, size=(5, 2))\n",
    "\n",
    "t3 = torch.multiply(t1, t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b145ca9-552a-4225-8892-dc2cf4436f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = torch.mean(t1, axis=0)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44225107-44e6-4853-86ed-4e8079bc7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = torch.matmul(t1, torch.transpose(t2, 0, 1))\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9aa56-a6c3-4370-b14c-91fea0244a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t6 = torch.matmul(torch.transpose(t1, 0, 1), t2)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999cb08-af40-46a3-8a0f-cd6b8f2326d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_t1 = torch.linalg.norm(t1, ord=2)\n",
    "print(norm_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8300a-6e69-4b4d-b554-8a2857c987da",
   "metadata": {},
   "source": [
    "**Module for managing your datasets** PyTorch offers a convenient module for loading and batching your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ecf2c-9c34-46ef-86ef-5ced0933e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "t = torch.arange(6, dtype=torch.float32)\n",
    "data_loader = DataLoader(t)\n",
    "\n",
    "for item in data_loader:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26889c4c-ef3a-4603-944c-ab3629224e30",
   "metadata": {},
   "source": [
    "We can also batch this dataset as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818a2f4-6c15-44fb-89e9-19b202288189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(t, batch_size=3, drop_last=False)\n",
    "\n",
    "for i, batch in enumerate(data_loader, 1):\n",
    "    print(f'batch {i}:', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f4ef6-3e8f-4b45-ba3f-4311a084fa65",
   "metadata": {},
   "source": [
    "**Training a linear regression model** Let's build up to implementing a deep learning model in PyTorch by implementing a linear regression model on some synthetic data. Here's the synthetic data we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1b08b-4d13-41a8-bb30-d103d970cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.arange(10, dtype='float32').reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, \n",
    "                    7.4, 8.0, 9.0], dtype='float32')\n",
    "\n",
    "plt.plot(X_train, y_train, 'o', markersize=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "#plt.savefig('figures/12_07.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c170f6cd-bad5-48d8-bead-6eb933b5000d",
   "metadata": {},
   "source": [
    "Let's do our usual pre-processing step of centering and normalizing the features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634eb5b-cc44-453a-b46a-455c5eff1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "batch_size = 1\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b47b2e-d0de-46a7-9005-dc94216e82a8",
   "metadata": {},
   "source": [
    "Now, let's implement our actual linear regression model. We'll see the reason for some of this code later, so don't expect to understand it 100% now. First, we'll initialize the weights and define the model (affine map) and loss function (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a310af-c072-4222-83ec-eef45c17b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "weight = torch.randn(1)\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    " \n",
    "def loss_fn(input, target):\n",
    "    return (input-target).pow(2).mean()\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weight + bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6142321-563b-46b7-9f74-55acc3a2956c",
   "metadata": {},
   "source": [
    "Then, we'll set the learning rate and iterate over the batches in the training dataset. Observe carefully how we use the `loss.backward()` call to compute the gradients and then use the gradients and the learning rates to update, we'll comment on this more below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca36def-4d7b-47f1-9dcc-f013bf8ded93",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "log_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            weight -= weight.grad * learning_rate\n",
    "            bias -= bias.grad * learning_rate\n",
    "            weight.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    " \n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch}  Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a30b8-51ba-4139-9389-a118c9919353",
   "metadata": {},
   "source": [
    "Let's check the final parameters and evaluate the model on a synthetic \"test\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17dc23a-187d-43ae-ba75-1aafa2332c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Parameters:', weight.item(), bias.item())\n",
    " \n",
    "X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)\n",
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm)\n",
    "y_pred = model(X_test_norm).detach().numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(X_train_norm, y_train, 'o', markersize=10)\n",
    "plt.plot(X_test_norm, y_pred, '--', lw=3)\n",
    "plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)\n",
    "ax.set_xlabel('x', size=15)\n",
    "ax.set_ylabel('y', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    " \n",
    "#plt.savefig('figures/12_08.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5af35-65be-41fd-a034-263c8c8c0b88",
   "metadata": {},
   "source": [
    "**The PyTorch neural networks module.** The `torch.nn` module is a powerful module for implementing neural networks. The following code does the same thing as above but with torch.nn objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe478724-756d-43ac-b122-983ceb42cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "# This is the torch.nn analog of our model method\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# This is the torch.nn analog of our loss_fn method\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# This sets up the SGD optimization\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        # 1. Generate predictions\n",
    "        pred = model(x_batch)[:, 0] \n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        # 3. Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Update parameters using gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5. Reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch}  Loss {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6d924-ec46-46a1-8161-29cde00bc785",
   "metadata": {},
   "source": [
    "Let's look at our final parameters and \"test\" performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d21fe-d697-4d1b-8985-933cc1675372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Parameters:', model.weight.item(), model.bias.item())\n",
    " \n",
    "X_test = np.linspace(0, 9, num=100, dtype='float32').reshape(-1, 1)\n",
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm)\n",
    "y_pred = model(X_test_norm).detach().numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(X_train_norm.detach().numpy(), y_train.detach().numpy(), 'o', markersize=10)\n",
    "plt.plot(X_test_norm.detach().numpy(), y_pred, '--', lw=3)\n",
    "plt.legend(['Training examples', 'Linear reg.'], fontsize=15)\n",
    "ax.set_xlabel('x', size=15)\n",
    "ax.set_ylabel('y', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a4c92-0c25-4c83-8129-382fdf2f7580",
   "metadata": {},
   "source": [
    "**Finally, applying `torch.nn` to an actual neural network.** It should now be straightforward to apply `torch.nn` to build basic two layer perceptron for the Iris datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb8a90-6a16-425b-b9a9-805e91e2497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1./3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efee959-2b0f-4a7a-bae4-6f9d67c8c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)\n",
    "X_train_norm = torch.from_numpy(X_train_norm).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "train_ds = TensorDataset(X_train_norm, y_train)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c2d07-8390-4916-ad76-413ce534a5b7",
   "metadata": {},
   "source": [
    "The following code specifies our model as a two-layer perceptron with a sigmoid hidden layer and softmax output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf90ed7-2178-4ff5-934e-252ef9d2a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    " \n",
    "model = Model(input_size, hidden_size, output_size)\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908cb2d-89b2-4aba-ad9b-67b5c105fbce",
   "metadata": {},
   "source": [
    "We'll use the cross-entropy loss, and the Adam optimization algorithm which is an augmented form of SGD (both of which we will unfortunately not have the time to get into)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caac7ae-b795-4994-b647-6e94d3917da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e305084-4992-4bce-af20-70f9720d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "loss_hist = [0] * num_epochs\n",
    "accuracy_hist = [0] * num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch.long())\n",
    "        # Compute the gradients, take a step in the optimization, and\n",
    "        # re-set the gradients to 0\n",
    "        ### YOUR CODE HERE ###\n",
    "    \n",
    "        loss_hist[epoch] += loss.item()*y_batch.size(0)\n",
    "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "        accuracy_hist[epoch] += is_correct.sum()\n",
    "        \n",
    "    loss_hist[epoch] /= len(train_dl.dataset)\n",
    "    accuracy_hist[epoch] /= len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78877454-81f9-4224-8a94-f599fde3a645",
   "metadata": {},
   "source": [
    "Let's look at the train performance over the optimization trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34788d1b-7917-4bc8-9920-5ac73d513091",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(loss_hist, lw=3)\n",
    "ax.set_title('Training loss', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(accuracy_hist, lw=3)\n",
    "ax.set_title('Training accuracy', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('figures/12_09.pdf')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134c554-1ac6-4859-a8d2-0fa7f096d175",
   "metadata": {},
   "source": [
    "And look at test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efee61b-7986-401a-858a-6b6c406930ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n",
    "X_test_norm = torch.from_numpy(X_test_norm).float()\n",
    "y_test = torch.from_numpy(y_test) \n",
    "pred_test = model(X_test_norm)\n",
    "\n",
    "correct = (torch.argmax(pred_test, dim=1) == y_test).float()\n",
    "accuracy = correct.mean()\n",
    " \n",
    "print(f'Test Acc.: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
