{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93e1122-2077-402f-af03-58ae2d23109b",
   "metadata": {},
   "source": [
    "**Implementing a two-layer perceptron.** In this coding session, we will get some practice implementing a two-layer perceptron. The code in this session is heavily borrowed from the [Machine Learning with PyTorch and Scikit-learn book's Github repo](https://github.com/rasbt/machine-learning-book/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3ded1-250f-4ac0-85a5-38877cca3409",
   "metadata": {},
   "source": [
    "**Notation for neural networks.** As in lecture, let's use the following notation for a neural network: let $(V, E, w)$ be a directed, weighted graph, and suppose the vertex set $V$ can be split into disjoint sets of vertices, $V = \\bigcup_{\\ell = 0}^L V_\\ell$ such that if $e \\in E$ then $e = (v, v')$ for $v \\in V_{\\ell}$ and $v' \\in V_{\\ell + 1}$ for some $\\ell = 0, 1, \\ldots, L - 1\\}$. The number $L$ is called the number of *layers* of the neural network. The integers $d_{\\rm in} := |V_0|$ and $d_{\\rm out} := |V_L|$ are called the *input* and *output* dimensions of the network. The parameter $w$ are the *weights*, which we can think of either as a function $w \\colon E \\to \\mathbb{R}$, or as a collection of matrices $(W_1, \\ldots, W_{L})$ where each $W_\\ell \\in \\mathbb{R}^{|V_{\\ell}| \\times |V_{\\ell - 1}|}$, along with biases $b_1, \\ldots, b_L$ such that $b_\\ell \\in \\mathbb{R}^{|V_{\\ell}|}$.\n",
    "\n",
    "We assume as given a non-linearity $\\sigma \\colon \\mathbb{R} \\to \\mathbb{R}$, for example a ReLU unit ($\\sigma(\\tau) := \\max(0, \\tau)$), a logistic unit ($\\sigma(\\tau) = (1 + e^{-\\tau})^{-1}$) or something else.\n",
    "\n",
    "The network then generates output via a *forward pass*. Given an input $x \\in \\mathbb{R}^{d_{\\rm in}}$, we calculate the output $o_0(x) = (x_1, \\ldots, x_d, 1)$, and then for subsequent layers we put $a_\\ell(x) = W_\\ell o_{\\ell - 1}(x) + b_\\ell$ and $o_{\\ell}(x) := \\sigma(a_{\\ell}(x))$ for output. The output of the network is, finally, $o_L(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41ad96-d2bd-460f-927a-f2754e7a8fc2",
   "metadata": {},
   "source": [
    "**Specializing to a two-layer perceptron.** In this lab, we will get into the nitty gritty details of a two-layer, fully connected, perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b789183-1b8d-4139-af69-f443b4f09f57",
   "metadata": {},
   "source": [
    "**Importing and preparing the MNIST dataset** We will use the MNIST dataset, which is a collection of images of handwritten digits (1, 2, ... , 9) half written by high school students and half written by employees of the US Census Bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0224e3-b209-46bd-9cb8-d8494523e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto')\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94179f06-9761-41fa-91b0-61b6b7c08c6a",
   "metadata": {},
   "source": [
    "The images in the MNIST dataset consist of 28×28 pixels, and each pixel is represented by a grayscale intensity value. Here, fetch_openml already unrolled the 28×28 pixels into one-dimensional row vectors, which are the rows in our X array (784 per row or image) above. The second array (y) returned by the fetch_openml function contains the corresponding target variable, the class labels (integers 0-9) of the handwritten digits.\n",
    "\n",
    "Next, let’s normalize the pixels values in MNIST to the range –1 to 1 (originally 0 to 255):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2fdd19-1eea-45ea-a870-fa26fa48a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X/255. - .5)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caf07a-7507-48cd-b918-cf9af8a65a25",
   "metadata": {},
   "source": [
    "We'll be using gradient descent based methods to train out model, and for these methods it is especially important that the features are normalized. Now let's visualize the first elements of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f66d32-d598-4537-b40d-8dfc18c3c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_4.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de945cd-b981-486a-ac49-6aee80fbd9a1",
   "metadata": {},
   "source": [
    "Now let's plot different 7s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5ba61-b8de-49ec-a4f8-0f51c85bd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X[y == 7][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/11_5.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd480c92-54fe-413b-9cbd-ebb621c0a66a",
   "metadata": {},
   "source": [
    "Finally, we will split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ad6f9-f994-48e7-8999-df8e92708fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\n",
    "\n",
    "# # optional to free up some memory by deleting non-used arrays:\n",
    "# del X_temp, y_temp, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931e485-5877-44e3-839d-a6354ccecce0",
   "metadata": {},
   "source": [
    "**Building a basic two-layer perceptron class.** We'll now work on implementing a basic two-layer neural network model. To this end, we'll need to handle a detail involving the encoding of the class labels. As they currently stand, the class labels are integers in $[9]$, but to work with them in a neural network model, we'll need to use an encoding to the output dimension. The encoding we'll use is known as a *one-hot* encoding, where the label $i \\in [9]$ is mapped to the $i + 1$st basis vector in $\\mathbb{R}^{10}$. The code for this encoding follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefebad-8020-46ff-bcfc-4bea21e6b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_onehot(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afafa8a-a298-4dc2-bb3e-49351017300e",
   "metadata": {},
   "source": [
    "**Affine forward.** Let's now start to implement the basic layers of our neural network, beginning with the forward pass for the affine layer: this is the layer which takes as input $X \\in \\mathbb{R}^{n \\times d}$ and maps it to $XW$ for $W \\in \\mathbb{R}^{d \\times m}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc6d10-6f9a-4780-87aa-421b3e1ec28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, D) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (D,).\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, D)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    # ***** START OF YOUR CODE *****\n",
    "\n",
    "    # ***** END OF YOUR CODE *****\n",
    "\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6007a8-e681-41ec-abd9-a909670aed46",
   "metadata": {},
   "source": [
    "Check that your implementation here is correct by using the following code. Check that the above is correct with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34615ab-8d28-41b0-8625-11e37c7b8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (120,)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c7913-5e4f-4086-98e2-a27e49078539",
   "metadata": {},
   "source": [
    "**Affine backward.** Now implement the backward pass for an affine layer. For this, it is helpful to think about what the following should be: if $g \\colon \\mathbb{R}^{l \\times n} \\to \\mathbb{R}$ and $A \\in \\mathbb{R}^{l \\times m}$, $B \\in \\mathbb{R}^{m \\times n}$, then what is\n",
    "$$\n",
    "\\nabla_Ag(AB) = \\, \\, ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc90f0-04ce-4c07-8495-e7832c739cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, D)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, D)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    \n",
    "    # ***** START OF YOUR CODE *****\n",
    "\n",
    "    # ***** END OF YOUR CODE *****\n",
    "    \n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cca56e-57d0-4b86-be17-b14e215fd310",
   "metadata": {},
   "source": [
    "Check that the above is correct with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a67cfb-e7c4-4bdf-afec-583ec88b29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 6)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347aafd8-e2c4-4358-ad83-1e27c0b55438",
   "metadata": {},
   "source": [
    "**ReLU forward.** Next, implement the ReLU forward pass. Recall that this is $\\mathrm{ReLU}(x) = \\max(0, x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec3da6-6dff-4a6c-8f58-a7bb4935ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    # ***** START OF YOUR CODE *****\n",
    "\n",
    "    # ***** END OF YOUR CODE *****\n",
    "\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb2cd6-0711-44f0-867d-c438c1c7abd8",
   "metadata": {},
   "source": [
    "Test your implementation of the forward ReLU operation with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084f366-4d18-405a-b2ad-301e07eeacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eba820-d47b-4f26-b76f-d09031540887",
   "metadata": {},
   "source": [
    "**ReLU backward.** Finally, implement the backward ReLU operation. Note that there's no need to worry about differentiability at $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a32eb-f9e8-428d-8123-c08e0e60586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "\n",
    "    # ***** START OF YOUR CODE *****\n",
    "\n",
    "    # ***** END OF YOUR CODE *****\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd716d-c1ff-4df5-b448-5f6352d4e1f5",
   "metadata": {},
   "source": [
    "Test your backward ReLU implementation with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6d9b2-d320-4459-98ce-5f9f43a58f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "sz = 10\n",
    "x = np.random.randn(sz, sz + 1)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55438097-7fe4-4d2f-a089-be1cd44f6848",
   "metadata": {},
   "source": [
    "The following code conveniently encapsulates the logical of an affine to ReLU layer, and should be a helpful reference for the implentation of ```TwoLayerNN``` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2ee18-c937-4651-8eb6-78d1a0ac371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Convenience layer that perorms an affine transform followed by a ReLU\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the affine-relu convenience layer\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0e9d9-6354-4de2-a0dd-d02e271f107e",
   "metadata": {},
   "source": [
    "Now that we've implemented our layers, let's make our  2 layer perceptron. Because it has two layers, its constructor will accept three integers, each corresponding to the number of nodes at each layer. An important detail to observe is that when we initialize the weights of our perceptron, we initialize them *randomly*. For now, we will ignore the details of the backpropogation which is necessary for training, and you will implement part of the initialization method as well as part of the forward propogation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb12c3-fd90-46c7-a45c-dd6a1be0dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network with sigmoid nonlinearity and\n",
    "    softmax loss that uses a modular layer design. We assume an input dimension\n",
    "    of D, a hidden dimension of H, and perform classification over C classes.\n",
    "\n",
    "    The architecure should be affine - relu - affine - softmax.\n",
    "\n",
    "    Note that this class does not implement gradient descent; instead, it\n",
    "    will interact with a separate Solver object that is responsible for running\n",
    "    optimization.\n",
    "\n",
    "    The learnable parameters of the model are stored in the dictionary\n",
    "    self.params that maps parameter names to numpy arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['weight_in'] = rng.normal(\n",
    "            loc=0.0, scale=0.001, size=(num_features, num_hidden))\n",
    "        self.params['bias_in'] = np.zeros(num_hidden)\n",
    "        \n",
    "        # Initialize second weight matrix\n",
    "        self.params['weight_out'] = rng.normal(\n",
    "            loc=0.0, scale=0.001, size=(num_hidden, num_classes))\n",
    "        self.params['bias_out'] = np.zeros(num_classes)\n",
    "\n",
    "    def loss(self, X, y=None): \n",
    "        \"\"\"\n",
    "        Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, D)\n",
    "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, C) giving classification scores, where\n",
    "          scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        if y is None:\n",
    "            out1, cache1 = affine_relu_forward(X, self.params['weight_in'], self.params['bias_in'])\n",
    "            out2, cache2 = affine_forward(out1, self.params['weight_out'], self.params['bias_out'])\n",
    "\n",
    "            return out2\n",
    "\n",
    "        if y is not None:\n",
    "            out1, cache1 = affine_relu_forward(X, self.params['weight_in'], self.params['bias_in'])\n",
    "            out2, cache2 = affine_forward(out1, self.params['weight_out'], self.params['bias_out'])\n",
    "            loss = None\n",
    "            grads = {}\n",
    "\n",
    "            # ***** START OF YOUR CODE ***** #\n",
    "\n",
    "            # ***** END OF YOUR CODE ***** #\n",
    "\n",
    "            grads['weight_in'] = dweight_in\n",
    "            grads['bias_in'] = dbias_in\n",
    "            grads['weight_out'] = dweight_out\n",
    "            grads['bias_out'] = dbias_out\n",
    "\n",
    "            return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d85c0-edd9-488b-a32f-510a79cd6d2b",
   "metadata": {},
   "source": [
    "Let's initialize our model; we'll make the hidden layer have 50 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56109e4-b852-477c-ab52-49eaf5de020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnn = TwoLayerNN(num_features=28*28, num_hidden=50, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084901b2-c9ad-4a0e-9b65-cdb81a8d1647",
   "metadata": {},
   "source": [
    "**Reminders on GD, SGD, and mini-batch SGD.** Now, we'll a form of gradient descent to train out model. Here's a brief reminder on gradient descent methods (we'll go into more detail in lectures).\n",
    "\n",
    "Suppose we want to solve the minimization problem\n",
    "$$\n",
    "\\min_{\\theta \\in \\mathbb{R}^d} f(\\theta).\n",
    "$$ The function $f$ is called the *objective*. Then gradient descent uses the iteration, for some initial point $\\theta_0$,\n",
    "$$\n",
    "\\theta_{t + 1} = \\theta_t - \\eta_t \\nabla f(\\theta_t),\n",
    "$$ where $\\eta_t \\in \\mathbb{R}$ is a *learning rate* that is specified by the user (typically a constant in the case of gradient descent proper).\n",
    "\n",
    "An objective $f$ that arises frequently in statistics and machine learning is of the form\n",
    "$$\n",
    "f(\\theta) = \\frac1n \\sum_{i = 1}^n \\ell(h_{\\theta}(x_i), y_i),\n",
    "$$ where $\\ell$ is a loss function and $(h_{\\theta})_{\\theta \\in \\Theta}$ is a parametrized model class. Indeed, this objective corresponds to an *empirical loss*. For example, in the setting of two layer perceptrons we'll use $\\ell(z , y) := \\| z - y \\|^2$ the $\\ell_2$-loss, and $h_{\\theta}$ the neural network with parameters $\\theta = (W_1, W_2, b_1, b_2)$, and the sum will be over the training data of MNIST. Often (and in our case), the $n$ will be very large, making direct computation of $\\nabla f(\\theta)$ impractical and thus ruling out the gradient descent algorithm.\n",
    "\n",
    "A work-around for this issue, which has additional attractive properties that we won't discuss now, is called *stochastic gradient descent (SGD)*. Here, we take a uniformly random $i \\in [n]$, and use it to form a *stochastic gradient*\n",
    "$$\n",
    "\\hat \\nabla f(\\theta) := \\nabla_\\theta \\ell(h_{\\theta}(x_i), y_i).\n",
    "$$ Observe that, over the randomness of this uniform random sample, $\\mathbb{E}[\\hat \\nabla f(\\theta)] = \\nabla f(\\theta)$. It is thus plausible that we can use this stochastic gradient as a replacement for the full gradient $\\nabla f(\\theta)$, and run a stochastic version of the gradient descent algorithm. This is called *stochastic gradient descent*, and uses the updates\n",
    "$$\n",
    "\\theta_{t + 1} = \\theta_t - \\eta_t \\hat \\nabla f(\\theta_t),\n",
    "$$ where, importantly, we use fresh randomness at each iteration to form the stochastic gradient $\\hat \\nabla f(\\theta_t)$.\n",
    "\n",
    "Finally, the SGD algorithm can be excessively noisy due to the fact that we use only one data point to form the stochastic gradient $\\hat \\nabla f(\\theta_t)$. This is remedied by averaging over several stochastic gradients to produce *mini-batch SGD*. Specifically, let $M$ be a fixed positive integer (the mini-batch size) and let $\\{i_1, \\ldots, i_M\\} \\subset [n]$ be a random subset of $M$ integers in $[n]$. Then we can form the averaged stochastic gradient\n",
    "$$\n",
    "\\bar \\nabla^M f(\\theta) := \\frac1M \\sum_{m = 1 }^M \\nabla_\\theta \\ell(h_{\\theta}(x_{i_m}), y_{i_m})\n",
    "$$ The minibatch SGD algorithm is then\n",
    "$$\n",
    "\\theta_{t + 1} = \\theta_t - \\eta_t \\bar \\nabla^M f(\\theta_t).\n",
    "$$ In practice, the way the randomness for the minibatch data or SGD is typically generated is by first randomly permuting the training data, and then iterating through the permuted data until there is no more data left, and then re-permuting the data and starting over again. Each iteration through the training data is referred to as an *epoch*.\n",
    "\n",
    "The following utility function (no need to read carefully) generates minibatches appropriate for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a02df0-dc6d-43aa-ad66-b60e1acd0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "minibatch_size = 400\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279d422-8d7b-4b07-a4e9-c33adaf384a9",
   "metadata": {},
   "source": [
    "Let's now define functions that compute the MSE and accuracy of our model, and check that they give reasonable numbers when run on our existing model (remember, we haven't trained it yet so it is merely randomly initialized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fac4d-ea1c-46f0-9e73-360c14f589b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return (0.5 / np.shape(targets)[0]) * np.linalg.norm(onehot_targets - probas)**2\n",
    "    \n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "probas = tnn.loss(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f736a2-ee34-4f0a-af79-4ea4f3aeacca",
   "metadata": {},
   "source": [
    "This is a plausible accuracy (about 1/10) since we initialized the weights randomly.\n",
    "\n",
    "Now, we'll use the minibatch method to implement a version of the above methods that will scale to very large data sets where we might otherwise run into memory issue. There's no need to look at this method in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506467b3-e75e-40cc-8434-abf095030607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        probas = nnet.loss(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = (0.5 / np.shape(targets)[0]) * np.linalg.norm(onehot_targets - probas)**2\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/(i+1)\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e531c-e5c4-47b6-91ab-b08bb3607c11",
   "metadata": {},
   "source": [
    "We'll get the same result as before using this large-scale version of the previous methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd5785-5f84-4413-9e54-f75baa655f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, acc = compute_mse_and_acc(tnn, X_valid, y_valid)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a0469-fd4f-4c98-a01c-240eb72c0ed3",
   "metadata": {},
   "source": [
    "We can now, finally, implement the training method! Note that we will be keeping track of the train and validation MSE and accuracy throughout the optimization so we can make plots below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683aefc9-60da-431f-8ac2-0376de4dc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "            mini_onehots = int_to_onehot(y_train_mini, num_labels=10)\n",
    "\n",
    "            # ***** START OF YOUR CODE ***** #\n",
    "\n",
    "            # ***** END OF YOUR CODE ***** #\n",
    "        \n",
    "        #### Epoch Logging ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af69961-33b2-42b7-8bb3-22395502bfe7",
   "metadata": {},
   "source": [
    "Let's see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e09a6b-78f7-4776-9e72-e5549ba8c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    tnn, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0069d9-c026-493a-a5aa-a97d700949d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccff41-1d3a-41d7-b5e3-12e99b8c575d",
   "metadata": {},
   "source": [
    "Now we can plot the accuracy of the training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b76dfc-fd7c-40b8-a369-600cf5e0bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_train_acc)), epoch_train_acc,\n",
    "         label='Training')\n",
    "plt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,\n",
    "         label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e73ac-90dc-4eca-bb67-1c3f8eea508f",
   "metadata": {},
   "source": [
    "We finally look at the ultimate test accuracy of the trained model, and plot some of the misclassified digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f523f-d695-4504-8cf2-c23fda130a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_acc = compute_mse_and_acc(tnn, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9143b-1a45-4110-b570-21f36897f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "probas = tnn.loss(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_09.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f647d-348e-49a3-9c1f-f636778bc006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
