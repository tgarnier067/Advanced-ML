{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817ef3af-76b7-43ca-8185-aca037e014f1",
   "metadata": {},
   "source": [
    "Welcome to our first TD session dedicated to coding!\n",
    "\n",
    "In this session we will implement the **Perceptron algorithm with kernels**. This code is mostly borrowed from the [Machine Learning with PyTorch and Scikit-learn book's Github repo](https://github.com/rasbt/machine-learning-book/tree/main).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7feb850-30c0-4391-a77f-747092da3716",
   "metadata": {},
   "source": [
    "**The Perceptron with bias.** In this lab, we'll be using a slightly modified form of the Perceptron algorithm, the Perceptron with *bias*.\n",
    "\n",
    "So suppose we are given data $(x_1, y_1), \\ldots, (x_n, y_n) \\in \\mathbb{R}^d \\times \\{\\pm 1\\}$. Then we wish to find a $w \\in \\mathbb{R}^d$ *and a bias $b \\in \\mathbb{R}$* such that $\\mathrm{sgn}(\\langle w, x_i \\rangle + b) = y_i$. The Perceptron with bias algorithm starts with $w^{(0)} = 0$ and $b^{(0)} = 0$, and uses the iterative update $w^{(t + 1)} = w^{(t)} + y_i x_i$ and $b^{(t + 1)} = b^{(t)} + y_i$. for some $(x_i, y_i)$ such that $\\mathrm{sgn}(\\langle w^{(t)}, x_i \\rangle + b^{(t)}) \\neq y_i$. If there is no such $(x_i, y_i)$, the algorithm terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9847cf3-5eaa-44f6-845d-5b7c95ff899d",
   "metadata": {},
   "source": [
    "**Implementing the Perceptron.** We'll now implement a basic version of the Perceptron algorithm, using an object-oriented approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce036f4-cc55-4972-97ad-3354138c4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Maximum passes over the training dataset.\n",
    "      \n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    b_ : Scalar\n",
    "      Bias unit after fitting.\n",
    "    errors_ : list\n",
    "      Number of misclassifications (updates) in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=1.0, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        self.w_ = np.zeros(X.shape[1])\n",
    "        self.b_ = np.float_(0.)\n",
    "        \n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                if target*self.predict(xi) < 0:\n",
    "                    self.w_ += self.eta * target * xi\n",
    "                    self.b_ += self.eta * target\n",
    "                    errors += 1\n",
    "            if errors == 0:\n",
    "                return self\n",
    "            \n",
    "            self.errors_.append(errors)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\" Calculate net input \"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Return class label after unit step \"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57292339-5d20-42a1-8403-41c45fdd26e1",
   "metadata": {},
   "source": [
    "Let's load some data to run our Perceptron on. We'll use a classic dataset, the Iris dataset, which consists of four length measurements (petal sepal length/width and petal length/width) for three species of Iris flower (Setosa, Versicolor, and Virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163196e-9890-4140-9c2a-300003c1cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(\"This object has several attributes...\\n\")\n",
    "\n",
    "print(\"A data attribute, which is an array of shape iris.data.shape = \" + str(iris.data.shape) + \"\\n\")\n",
    "\n",
    "print(\"A target attribute, which is an array with 150 entries corresponding to each class iris.target = \" + str(iris.target) + \"\\n\")\n",
    "\n",
    "print(\"A feature names attribute iris.feature_names = \" + str(iris.feature_names) + \"\\n\")\n",
    "\n",
    "print(\"A target names attribute iris.target_names = \" + str(iris.target_names) + \"\\n\")\n",
    "\n",
    "print(\"Finally, we can look at some elements of iris.data: \\n\" + str(iris.data[0:5, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f9406-87c1-4fc7-aa61-ded560972d48",
   "metadata": {},
   "source": [
    "Let's plot the data. We'll just be looking at the sepal length and petal length dimensions of the data since it's easiest to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb640f-474d-4743-a7a0-1f20d14a4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris.data[:, 0], iris.data[:, 2], c=iris.target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[2])\n",
    "_ = ax.legend(\n",
    "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816dec6-db78-4471-a935-19a5703625e8",
   "metadata": {},
   "source": [
    "As you can see, we won't be able to find a perfect linear classifier if we try to classify Versicolor vs. Virginica. So let's do the following manipulations: \n",
    "\n",
    "- remove the Virginica samples\n",
    "- remove the features corresponding to sepal and petal width\n",
    "- create the vector of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3877656-c16d-4a96-89fb-581f304661ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the feature array of shape (100, 2), first 50 elements are setosa\n",
    "# sepal length and petal length, next 50 elements are Versicoloa sepal\n",
    "# length and petal lemngth\n",
    "X = ### YOUR CODE HERE ###\n",
    "\n",
    "# y is the corresponding class vector, first 50 elements are +1 and last 50 elements are -1\n",
    "y = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ee7ee-63d7-4981-8e50-1f6c21cb839e",
   "metadata": {},
   "source": [
    "Now, make a scatter plot similar to the above but with the X and y arrays you just made, so we only see the Setosa and Versicolor samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f487b-0f34-4325-ad9f-1ecbdddf1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e70ae-6d39-4f2f-90f4-099f17bc2c2d",
   "metadata": {},
   "source": [
    "Let's finally run our Perceptron!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525e8f4-73d6-4ac3-87cd-1e99b82b12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Perceptron and fit it\n",
    "P = ### YOUR CODE HERE ###\n",
    "\n",
    "# Inspect the weights, bias, and errors\n",
    "w = ### YOUR CODE HERE ###\n",
    "b = ### YOUR CODE HERE ###\n",
    "errors = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19fd4b-f0ee-440a-8069-57c2bcf4c176",
   "metadata": {},
   "source": [
    "Now make a plot of the errors versus the passes over the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec4aa0-69e5-4fe2-89c3-726100f243bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26d9bf-7dec-4006-a5f7-c4593ebf5126",
   "metadata": {},
   "source": [
    "It'll be useful to have the following function which plots the decision boundaries of classifiers (no need to look carefully at this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693b394-a201-4bf5-9a5f-3ac0330d633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=f'Class {cl}', \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2138d-efc8-4528-a564-bd740e75e4f7",
   "metadata": {},
   "source": [
    "We'll use it with our trained Perceptron now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b8fbb-2759-4d37-827d-076d491ccda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier=P)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.legend([iris.target_names[1], iris.target_names[0]], loc=\"lower right\", title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58917ed7-38a1-482d-8691-72c6abb20d00",
   "metadata": {},
   "source": [
    "**You'll now implement a modified Perceptron, which accepts a kernel in the following manner.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca887bbe-6f09-41b4-a401-d2ef3f36cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_standard(u, v):\n",
    "    return u.dot(v.T)\n",
    "\n",
    "class KernelPerceptron:\n",
    "    \"\"\" Kernel Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    K : Kernel function\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    b_ : Scalar\n",
    "      Bias unit after fitting.\n",
    "    errors_ : list\n",
    "      Number of misclassifications (updates) in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=1.0, n_iter=50, K = K_standard):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.K = K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for k in range(0, self.n_):\n",
    "                ### YOUR CODE HERE ###\n",
    "\n",
    "            if errors == 0:\n",
    "                return self\n",
    "            \n",
    "            self.errors_.append(errors)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def net_input(self, x):\n",
    "        \"\"\" Calculate net input \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Return class label after unit step \"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832ff25-b8fe-41f6-933a-38252bf327a1",
   "metadata": {},
   "source": [
    "Let's check that the implementation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88199c5-b0e2-460e-9ad4-3f42e262075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KernelP_standard = KernelPerceptron()\n",
    "KernelP_standard.fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, classifier=KernelP_standard)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.legend([iris.target_names[1], iris.target_names[0]], loc=\"lower right\", title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5608273-d08e-4d12-af55-23801b21414f",
   "metadata": {},
   "source": [
    "You should recover the same plot as before.\n",
    "\n",
    "** Let's now consider some data which can't be linearly classified. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499e2fd-1978-4735-bd75-03046d3e7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X_circle, y_circle = make_circles(n_samples = 200, factor=.5)\n",
    "y_circle = 2*y_circle - 1 ## convert from 0-1 labels to -1 - 1 labels\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(X_circle[:, 0], X_circle[:, 1], c=y_circle)\n",
    "#ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[2])\n",
    "# plt.legend([\"Inner circle\", \"Outer circle\"], loc='upper-left', title=\"Classes\")\n",
    "# # _ = ax.legend(\n",
    "# #     scatter.legend_elements()[0], loc=\"lower right\", title=\"Classes\"\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b263d-72cb-4672-a8b9-ab756628974c",
   "metadata": {},
   "source": [
    "What's a kernel that can linearly separate this data? Use the code we have developed above to fit a Kernel Perceptron which achieves perfect accuracy on this dataset, and visualize its decision regions on the circle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b99ef-89a4-4e59-a828-9adf73e134b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_circle(u, v):\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "# Create and fit your Perceptron\n",
    "KernelP_circle = ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Visualize the decision regions\n",
    "\n",
    "plot_decision_regions(### YOUR CODE HERE ### )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
